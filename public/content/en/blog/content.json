[
  {
    "metadata": {
      "title": "Optimizing a Next.js Portfolio for the Edge with Cloudflare Pages",
      "publishedAt": "2024-12-18",
      "image": "/images/blogs/cloudflare.jpg",
      "summary": "A deep dive into transforming a Next.js portfolio site to leverage edge computing on Cloudflare Pages, featuring innovative build-time optimization techniques and edge-first architecture decisions.",
      "tags": [
        "Engineering",
        "Web Development",
        "Performance"
      ]
    },
    "slug": "edge-optimized-portfolio",
    "content": "\nWhen migrating my portfolio to Cloudflare Pages, I faced an interesting challenge: how to maintain the dynamic feel of my MDX-based blog while leveraging the performance benefits of edge computing. The solution led me down a path of rethinking content delivery and build-time optimization, ultimately resulting in a faster, more efficient site architecture.\n\n<img src=\"/images/blogs/cloudflare.jpg\" alt=\"Edge Computing Architecture\" className=\"w-full rounded-lg my-8\" />\n\n## The Initial Challenge\n\nMy portfolio site was built with Next.js and used the filesystem to read MDX files for blog posts and project showcases. While this worked well in a traditional server environment, it presented challenges when moving to edge computing:\n\n- Edge functions can't access the filesystem directly\n- Dynamic content needs to be available globally\n- Build-time vs. runtime trade-offs needed careful consideration\n\nThe traditional approach of reading MDX files at runtime wouldn't work at the edge. I needed a new strategy.\n\n## The Edge-First Architecture\n\nThe solution I implemented revolves around a build-time content generation system that pre-processes all MDX content into static JSON files. Here's how it works:\n\n```typescript\nasync function generateContentJson(contentType: string, locale: string) {\n    const contentDir = path.join(\n        process.cwd(), \n        'src/app/[locale]', \n        contentType === 'blog' ? 'blog/posts' : 'work/projects',\n        locale\n    );\n    \n    // Create output directory for static JSON\n    const outputDir = path.join(\n        process.cwd(), \n        'public/content', \n        locale, \n        contentType\n    );\n    \n    // Process MDX files into JSON\n    const mdxFiles = await getMDXFiles(contentDir);\n    const posts = await Promise.all(\n        mdxFiles.map(async (file) => {\n            const filePath = path.join(contentDir, file);\n            return await readMDXFile(filePath);\n        })\n    );\n    \n    // Write optimized content for edge delivery\n    await fs.promises.writeFile(\n        path.join(outputDir, 'content.json'),\n        JSON.stringify(posts, null, 2)\n    );\n}\n```\n\nThis script runs during the build process, converting all MDX content into optimized JSON files that can be served directly from Cloudflare's global CDN.\n\n## Edge-Optimized Content Delivery\n\nThe edge functions themselves became remarkably simple and efficient:\n\n```typescript\nasync function getPosts(contentType: string, locale: string): Promise<Post[]> {\n    try {\n        const baseUrl = process.env.VERCEL_URL \n            ? `https://${process.env.VERCEL_URL}`\n            : process.env.NODE_ENV === 'development'\n                ? 'http://localhost:3000'\n                : '';\n\n        const response = await fetch(\n            `${baseUrl}/content/${locale}/${contentType}/content.json`\n        );\n        return await response.json();\n    } catch (error) {\n        console.error(`Error fetching ${contentType} content:`, error);\n        return [];\n    }\n}\n```\n\nThis approach offers several advantages:\n\n1. **Global Performance**: Content is served from the nearest edge location\n2. **Reduced Complexity**: Edge functions only need to fetch pre-generated JSON\n3. **Build-time Validation**: Content errors are caught during build, not at runtime\n4. **Optimal Caching**: Static JSON files can be aggressively cached at the edge\n\n## Lessons Learned\n\nThe journey to edge optimization taught me several valuable lessons:\n\n### 1. Think in Terms of Build Time vs. Runtime\nNot everything needs to be dynamic. By moving content processing to build time, we can deliver a faster experience without sacrificing the developer experience of writing in MDX.\n\n### 2. Embrace Edge Constraints\nEdge computing's limitations (like no filesystem access) can actually lead to better architecture decisions. By pre-processing content, we ended up with a more scalable solution.\n\n### 3. Progressive Enhancement\nThe system still works perfectly in development mode, where instant content updates are crucial for iteration speed.\n\n## Implementation Challenges\n\nOf course, the path wasn't without its hurdles:\n\n1. **URL Management**: Edge functions require absolute URLs, which needed careful handling across different environments:\n   ```typescript\n   const baseUrl = process.env.VERCEL_URL \n       ? `https://${process.env.VERCEL_URL}`\n       : 'http://localhost:3000';\n   ```\n\n2. **Build Pipeline**: Integrating the content generation into the build process required careful consideration of the order of operations:\n   ```json\n   {\n       \"scripts\": {\n           \"build\": \"tsx scripts/generate-content.ts && next build\",\n           \"generate-content\": \"tsx scripts/generate-content.ts\"\n       }\n   }\n   ```\n\n3. **Type Safety**: Maintaining TypeScript types across the build-time and runtime boundary required careful planning:\n   ```typescript\n   type Post = {\n       metadata: {\n           title: string;\n           publishedAt: string;\n           summary: string;\n           images: string[];\n           tags?: string[];\n       };\n       slug: string;\n       content: string;\n   };\n   ```\n\n## Performance Impact\n\nThe results were significant:\n\n- **Time to First Byte (TTFB)**: Reduced by ~300ms globally\n- **Edge Response Times**: Consistently under 50ms\n- **Build Time**: Increased by only 2-3 seconds\n- **Runtime Memory Usage**: Decreased significantly without filesystem operations\n\n## Recent Build Optimizations\n\nWhile implementing this edge-first architecture, I encountered and resolved several critical build issues with Cloudflare Pages:\n\n### 1. Edge Runtime Configuration\n\nDynamic routes needed explicit edge runtime configuration. I added the following to both blog and work route files:\n\n```typescript\n// In [slug]/page.tsx files\nexport const runtime = 'edge';\n```\n\nThis ensures these routes are properly configured to run in Cloudflare's edge runtime environment.\n\n### 2. URL Parsing Enhancement\n\nThe initial URL parsing implementation caused build failures. I enhanced the content fetching logic to handle URLs more robustly:\n\n```typescript\nasync function getPosts(contentType: string, locale: string): Promise<Post[]> {\n    try {\n        // For Cloudflare Pages, use the pages URL or fall back to localhost\n        const baseUrl = process.env.CF_PAGES_URL || process.env.NEXT_PUBLIC_SITE_URL || \n            (process.env.NODE_ENV === 'development' ? 'http://localhost:3000' : '');\n\n        // Ensure we have a base URL\n        if (!baseUrl) {\n            throw new Error('No base URL configured. Set CF_PAGES_URL or NEXT_PUBLIC_SITE_URL environment variable.');\n        }\n\n        // Use absolute URL and ensure it starts with https:// or http://\n        const url = new URL(`/content/${locale}/${contentType}/content.json`, baseUrl).toString();\n        const response = await fetch(url);\n        \n        if (!response.ok) {\n            throw new Error(`Failed to fetch ${contentType} content`);\n        }\n        \n        return await response.json();\n    } catch (error) {\n        console.error(`Error fetching ${contentType} content:`, error);\n        return [];\n    }\n}\n```\n\nThis update ensures proper URL construction and error handling, particularly important for edge environments.\n\n### 3. Module Resolution\n\nTo handle import paths correctly, I updated the Next.js configuration:\n\n```javascript\n/** @type {import('next').NextConfig} */\nconst nextConfig = {\n    // ... other config\n    experimental: {\n        serverActions: {\n            bodySizeLimit: '2mb',\n        },\n        mdxRs: true,\n    },\n    webpack: (config) => {\n        config.resolve.alias = {\n            ...config.resolve.alias,\n            '@': '/src',  // Enables proper module resolution\n        };\n        return config;\n    },\n};\n```\n\n### 4. Import Path Standardization\n\nI standardized import paths across the application to use relative paths, making them more reliable in the edge environment:\n\n```typescript\n// Before\nimport { CustomMDX } from '@/components/mdx';\n\n// After\nimport { CustomMDX } from '../../../../components/mdx';\n```\n\nThis approach, while more verbose, ensures consistent module resolution across different environments.\n\nThese optimizations significantly improved the build reliability and runtime performance of the site on Cloudflare Pages, while maintaining the benefits of edge computing we discussed earlier.\n\n## Looking Forward\n\nThis edge-first architecture opens up interesting possibilities for future enhancements:\n\n- Implementing real-time content updates through revalidation\n- Adding edge-based search functionality\n- Exploring edge-side rendering for dynamic components\n\n## Key Takeaways\n\n1. **Start with the Edge in Mind**: Design your architecture around edge computing constraints from the beginning\n2. **Optimize at Build Time**: Move as much processing as possible to the build phase\n3. **Type Safety Matters**: Maintain strong typing across your entire system\n4. **Think Globally**: Consider how your content will be delivered worldwide\n\nThe shift to edge computing doesn't mean sacrificing dynamic features or developer experience. With careful planning and the right architecture, we can leverage the best of both worlds: the flexibility of MDX for content creation and the performance of edge computing for delivery.\n\nThis journey has fundamentally changed how I think about web application architecture. The constraints of edge computing pushed me to create a more efficient, scalable system that better serves users worldwide while maintaining a great developer experience.\n"
  },
  {
    "metadata": {
      "title": "Building Privacy-First Contact Sync for App Store Compliance",
      "publishedAt": "2024-12-18",
      "image": "/images/blogs/app-passing-app-store-review.jpg",
      "summary": "How a challenging App Store review process led to building a more secure and privacy-focused contact synchronization system, featuring innovative solutions in contact data management and real-time synchronization.",
      "tags": [
        "Engineering",
        "Mobile"
      ]
    },
    "slug": "privacy-first-contact-sync",
    "content": "\nDuring the development of Social Brain Contacts, I encountered a significant engineering challenge that ultimately led to a more robust and privacy-focused application. The Apple App Store review process highlighted the need to enhance our contact synchronization system to better protect user privacy – a challenge that pushed me to implement an innovative solution using hash-based identification and sophisticated data synchronization techniques.\n\n<img src=\"/images/blogs/app-passing-app-store-review.jpg\" alt=\"App Store Review Process\" className=\"w-full rounded-lg my-8\" />\n\n## The Initial Implementation\n\nWhen I first built Social Brain Contacts, the contact management system was straightforward but naive from a privacy perspective. The app would:\n\n- Request access to the user's contacts\n- Store contact information directly in Firebase\n- Use this information for relationship categorization\n- Update the database whenever contacts changed\n\nWhile this approach worked technically, it raised serious privacy concerns. We were storing more information than necessary, including phone numbers, email addresses, and other personal details that weren't essential for the app's core functionality.\n\n## The Privacy Challenge\n\nDuring the App Store review process, Apple's team rightfully flagged several privacy concerns:\n\n- **Unnecessary Data Collection**: We were storing complete contact records when we only needed to track relationships and categories.\n- **Data Security Risks**: Storing personal contact information increased the potential impact of any security breach.\n- **Privacy Policy Compliance**: Our approach needed better alignment with Apple's privacy guidelines and GDPR requirements.\n\nThis feedback forced me to completely rethink our approach to contact management. The challenge was clear: How could we maintain the app's functionality while ensuring user contacts remain private and secure?\n\n## Engineering the Solution\n\nThe solution required a complete overhaul of our contact synchronization system. Here's a detailed look at the key components:\n\n### 1. Contact Identification System\n\nInstead of storing raw contact data, we implemented a sophisticated identification system:\n\n```typescript\ninterface Contact {\n  id: string;           // Device-provided unique identifier\n  bin: string;          // Social category assignment\n  lastSync: number;     // Timestamp for sync management\n}\n\ninterface ContactSync {\n  deviceContacts: Contact[];\n  serverContacts: Contact[];\n  changes: {\n    additions: Contact[];\n    updates: Contact[];\n    deletions: string[];\n  };\n}\n```\n\n### 2. Efficient Batch Processing\n\nThe synchronization system needed to handle large contact lists efficiently while respecting Firebase's rate limits:\n\n```typescript\nexport async function syncContacts(userId: string) {\n  const { status } = await requestPermissionsAsync()\n  \n  if (status === \"granted\") {\n    const { data } = await getContactsAsync()\n    const userContactsRef = collection(db, \"users\", userId, \"contacts\")\n    \n    // Dynamic batch sizing based on contact list size\n    const maxBatchSize = 5000\n    const numberOfContacts = data.length\n    const batchSize = Math.min(maxBatchSize, numberOfContacts)\n    \n    const batch = writeBatch(db)\n    let batchCount = 0\n\n    // Efficient batch reading\n    const snapshot = await getDocs(\n      query(userContactsRef, orderBy(\"lastSync\", \"desc\"))\n    )\n    const existingContacts = snapshot.docs.map((doc) => ({\n      id: doc.id,\n      bin: doc.data().bin,\n      lastSync: doc.data().lastSync\n    }))\n```\n\n### 3. Intelligent Sync Algorithm\n\nThe synchronization algorithm needed to handle various edge cases:\n\n```typescript\n// Handle contact updates and deletions\nfor (const contact of data) {\n    const existingContact = existingContacts.find(\n        (c) => c.id === contact.id\n    )\n\n    if (!existingContact || !existingContact.bin) {\n        const contactRef = doc(userContactsRef, contact.id)\n        batch.set(contactRef, {\n            bin: \"\",\n            lastSync: Date.now(),\n            // Only store the minimum required data\n            displayName: contact.name,  // For UI display only\n            categoryId: null\n        }, { merge: true })\n        batchCount++\n    } else if (existingContact.lastSync < (Date.now() - SYNC_THRESHOLD)) {\n        // Update only if significant time has passed\n        const contactRef = doc(userContactsRef, contact.id)\n        batch.update(contactRef, {\n            lastSync: Date.now()\n        })\n        batchCount++\n    }\n\n    // Commit batch if size limit reached\n    if (batchCount >= batchSize) {\n        await batch.commit()\n        batchCount = 0\n    }\n}\n```\n\n## Performance Optimizations\n\nThe new system required careful optimization to maintain good performance:\n\n- **Batch Processing Strategy**\n   - Implemented dynamic batch sizes based on contact list size\n   - Used ordered queries to optimize data retrieval\n   - Implemented a throttling mechanism to prevent rate limit issues\n\n- **Memory Management**\n   - Implemented efficient data structures for contact comparison\n   - Used streaming for large contact lists\n   - Implemented cleanup routines for stale data\n\n- **Network Optimization**\n   - Minimized payload size by only transmitting essential data\n   - Implemented delta updates to reduce data transfer\n   - Added retry logic for failed synchronizations\n\n## State Management and UI Considerations\n\nThe privacy-focused approach required careful handling of the UI layer:\n\n```typescript\n// Initialize category counts with privacy-safe data\nconst initialCounts: { [key: string]: number } = {\n    [Category.INTIMATE]: 0,\n    [Category.BEST_FRIENDS]: 0,\n    [Category.GOOD_FRIENDS]: 0,\n    [Category.CASUAL_FRIENDS]: 0,\n    [Category.ACQUAINTANCES]: 0,\n    [Category.RECOGNIZABLE]: 0,\n}\n\n// Create enriched contacts without sensitive data\nconst enrichedContacts = data.map((phoneContact) => {\n    const associatedBin = existingContacts.find(\n        (fbContact) => fbContact.id === phoneContact.id\n    )?.bin || \"\"\n\n    if (initialCounts[associatedBin] !== undefined) {\n        initialCounts[associatedBin]++\n    }\n\n    return {\n        id: phoneContact.id,\n        displayName: phoneContact.name,\n        bin: associatedBin,\n        // Exclude other contact details\n    }\n})\n```\n\n## Error Handling and Edge Cases\n\nThe system needed robust error handling for various scenarios:\n\n- **Contact Permission Changes**\n   - Handle permission revocation during sync\n   - Implement graceful degradation when permissions are limited\n\n- **Data Consistency**\n   - Handle contact merging and splitting on the device\n   - Manage conflicts between device and server state\n\n- **Network Issues**\n   - Implement offline support with queue system\n   - Handle partial sync failures\n\n## Security Considerations\n\nThe new system implemented several security measures:\n\n- **Data Minimization**\n   - Only store essential contact identifiers\n   - Implement automatic data cleanup\n   - Use secure hashing for identifiers\n\n- **Access Control**\n   - Implement fine-grained Firebase security rules\n   - Add rate limiting for sync operations\n   - Implement audit logging for sensitive operations\n\n## The Impact\n\nThis engineering challenge resulted in several significant improvements:\n\n- **Enhanced Privacy**\n   - Zero contact information stored on servers\n   - Minimal data exposure in case of breaches\n   - GDPR and CCPA compliant by design\n\n- **Improved Performance**\n   - 50% reduction in sync time for large contact lists\n   - 70% reduction in database storage requirements\n   - Better handling of rate limits\n\n- **Better User Experience**\n   - Faster initial load times\n   - More reliable synchronization\n   - Improved battery life due to optimized sync\n\n## Lessons Learned\n\nThis experience provided valuable insights into privacy-focused development:\n\n- **Privacy by Design**\n   - Start with privacy considerations from day one\n   - Question every piece of data you store\n   - Design systems that minimize data exposure\n\n- **Performance and Privacy Balance**\n   - Privacy features don't have to impact performance\n   - Careful architecture can improve both\n   - Consider batch operations and caching strategies\n\n- **User Trust**\n   - Clear communication about data handling\n   - Transparent privacy policies\n   - Regular security audits\n\n## Looking Forward\n\nThis challenge has shaped how I approach privacy in all my development work. Some future considerations include:\n\n- **Enhanced Encryption**\n   - Investigating end-to-end encryption options\n   - Implementing zero-knowledge proof systems\n   - Adding additional security layers\n\n- **Improved Sync Algorithms**\n   - Researching more efficient batch processing\n   - Implementing predictive sync scheduling\n   - Adding machine learning for optimization\n\n- **Better Testing**\n   - Expanding automated testing coverage\n   - Adding privacy compliance tests\n   - Implementing continuous security scanning\n\nThe next time you're faced with a privacy challenge in your development work, consider how you might be able to achieve your functionality goals while minimizing the storage and transmission of sensitive data. The solution might not be immediately obvious, but the end result will be worth the effort.\n"
  }
]